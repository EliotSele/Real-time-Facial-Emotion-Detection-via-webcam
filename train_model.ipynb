{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading Tensorflow version 2.10.1**"
      ],
      "metadata": {
        "id": "QOvCt2i1Jgz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install TensorFlow 2.10.1\n",
        "!pip install tensorflow==2.10.1"
      ],
      "metadata": {
        "id": "sesIn-3HJm61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC6wuWe4-5vJ"
      },
      "source": [
        "**Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lltHtsIQ-9Cu"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping ,ModelCheckpoint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "# # Change directory\n",
        "# cascade_path = pathlib.Path(__file__).parent.absolute()\n",
        "# print(f\"cascade_path: {cascade_path}\")\n",
        "# os.chdir(cascade_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Google Drive in Google Colab to access files stored in Google Drive**"
      ],
      "metadata": {
        "id": "uPd-Ul24tCQx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvURL_pUaAQv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UW0y72bGlHW"
      },
      "source": [
        "**Installing kaggle and Fer2013 dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDDdaRb_GuHo"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check if the kaggle.json file is present in the /content directory**"
      ],
      "metadata": {
        "id": "xlc1MmFGmOsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To access the fer2013 dataset, follow these steps:\n",
        "# 1. Obtain your 'kaggle.json' file by visiting 'https://www.kaggle.com/settings>ÙŽAPI>Create new token'.\n",
        "# 2. Create a new token to generate the 'kaggle.json' file.\n",
        "# 3. Use this 'kaggle.json' file to access to Fer2013 dataset.\n",
        "\n",
        "if not os.path.exists('/content/kaggle.json'):\n",
        "    print(\"Please upload the kaggle.json file to the /content directory.\")\n",
        "    from google.colab import files\n",
        "    files.upload()\n",
        "else:\n",
        "    print(\"Already uploaded.\")"
      ],
      "metadata": {
        "id": "_0Vnp3x_mcVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downloading fer2013 dataset from Kaggle using kaggle API**"
      ],
      "metadata": {
        "id": "PNZffzx9jGKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdyhVEVsS-eI"
      },
      "outputs": [],
      "source": [
        "# Make sure the .kaggle directory exists in the /content directory\n",
        "!mkdir -p /content/kaggle\n",
        "# Copy the kaggle.json file to the .kaggle directory\n",
        "!cp /content/kaggle.json /content/kaggle/kaggle.json\n",
        "# List the files in the /content/kaggle directory to verify the copy\n",
        "!ls /content/kaggle\n",
        "# List the files in the /content/datasets directory to check if the datasets folder exists\n",
        "!ls /content/datasets\n",
        "# Set appropriate permissions for the kaggle.json file\n",
        "!chmod 600 /content/kaggle/kaggle.json\n",
        "# Set the KAGGLE_CONFIG_DIR environment variable to /content/kaggle\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/kaggle'\n",
        "# Verify the content of the .kaggle directory\n",
        "!ls /content/kaggle\n",
        "# Now you can use the Kaggle API without authentication issues\n",
        "!kaggle datasets list\n",
        "# Check if the fer2013 file is present in the /content/datasets directory\n",
        "if not os.path.exists('/content/datasets/fer2013.csv'):\n",
        "    # Make sure the datasets directory exists in the /content directory\n",
        "    !mkdir -p /content/datasets\n",
        "    # Download the desired Kaggle dataset to the /content/datasets directory\n",
        "    !kaggle datasets download -d deadskull7/fer2013 -p /content/datasets/\n",
        "    # Unzip the downloaded dataset\n",
        "    !unzip /content/datasets/fer2013.zip -d /content/datasets\n",
        "    # List the files in the /content/datasets directory to verify the download\n",
        "    !ls /content/datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXnDslXn_CCB"
      },
      "source": [
        "**Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rqajwi0_FsK"
      },
      "outputs": [],
      "source": [
        "# Preprocess the dataset\n",
        "df = pd.read_csv('/content/datasets/fer2013.csv')\n",
        "# df = pd.read_csv('fer2013.csv')\n",
        "\n",
        "# Printing columns of the dataset\n",
        "columns = df.columns\n",
        "print(f\"columns of the dataset: {columns}\")\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_data = df.loc[df['Usage'] == 'Training']\n",
        "test_data = df.loc[df['Usage'] == 'PrivateTest']\n",
        "\n",
        "# Get the training images\n",
        "x_train = np.array(list(map(str.split, train_data['pixels']))).astype(float)\n",
        "# Get the training labels\n",
        "y_train = train_data['emotion'].values\n",
        "\n",
        "# Get the test images\n",
        "x_test = np.array(list(map(str.split, test_data['pixels']))).astype(float)\n",
        "# Get the test labels\n",
        "y_test = test_data['emotion'].values\n",
        "\n",
        "num_classes = 7\n",
        "\n",
        "# Reshape the training and test images\n",
        "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1).astype('float32')\n",
        "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1).astype('float32')\n",
        "\n",
        "# Normalize the training and test images\n",
        "x_train /= 255.0\n",
        "x_test /= 255.0\n",
        "\n",
        "# Convert the training and test labels to categorical variables\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRcOZYleRdbx"
      },
      "source": [
        "**Define data augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RwlrhhPWReNf"
      },
      "outputs": [],
      "source": [
        "# Define data augmentation\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    # rotation_range=7,\n",
        "    # width_shift_range=0.1,\n",
        "    # height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    # vertical_flip=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I3ecXl__N5S"
      },
      "source": [
        "**Defining the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GM2FZYWW_Rsy"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "model = Sequential([\n",
        "    Input(shape=(48, 48, 1)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(16, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    BatchNormalization(),\n",
        "\n",
        "    Flatten(),\n",
        "    BatchNormalization(),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(8, activation='relu'),\n",
        "    # Dropout(0.2),\n",
        "    Dense(7, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "initial_learning_rate = 0.001\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "# # Define the learning rate scheduler\n",
        "# def scheduler(epoch, learning_rate):\n",
        "#     if epoch < 8:\n",
        "#         return learning_rate\n",
        "#     elif 7<epoch<50:\n",
        "#         return learning_rate * tf.math.exp(-0.001)\n",
        "#     elif epoch>49:\n",
        "#         return learning_rate * tf.math.exp(-0.01)\n",
        "\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqfPhjIO_Yuz"
      },
      "source": [
        "**Fitting the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LITnc_c_cv6"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "# lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "# Define the early stopping and model checkpoint callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# Save the best model\n",
        "model_checkpoint = ModelCheckpoint('/content/best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "history = model.fit(\n",
        "    data_augmentation.flow(x_train, y_train, batch_size=batch_size),\n",
        "    # x_train, y_train,\n",
        "    # batch_size=batch_size,\n",
        "    # steps_per_epoch=len(x_train) / batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[ early_stop, model_checkpoint] #lr_scheduler,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fCqsIGj_e5K"
      },
      "source": [
        "**Saving the model with the latest accuracy obtained**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kOWWMqOy_inC"
      },
      "outputs": [],
      "source": [
        "filename = 'emotion_detection_model.h5'\n",
        "filepath = '/content/' + filename\n",
        "model.save(filepath)\n",
        "# model.save('emotion_detection_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s_2CQEo4cC0"
      },
      "source": [
        "**Plot the training and validation loss and accuracy values over the epochs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBh5pDIr4jV7"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(4, 2))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(4, 2))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}